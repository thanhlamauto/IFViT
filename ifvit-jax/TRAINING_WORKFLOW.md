# IFViT Training Workflow - The Correct Way

This document explains the **correct workflow** for training IFViT following the paper exactly.

## âš ï¸ Important: Module 1 â†’ Module 2 Flow

**IFViT Paper Quote:**
> "The second module ... employs the ViTs **trained in the first module** with the additional fully connected layer and retrains them ..."

**Key Point:** Module 2 does NOT load LoFTR weights directly. It loads the TRAINED transformer from Module 1!

## ðŸ“Š Workflow Diagram

```
LoFTR Pretrained â†’ Module 1 (Train) â†’ Module 2 (Train) â†’ Final Model
    (.npz)         Dense Reg + L_D    Matcher + L_D+L_E+L_A
```

## ðŸ”„ Detailed Steps

### Step 1: Prepare LoFTR Pretrained Weights

```bash
# Download LoFTR checkpoint
wget https://github.com/zju3dv/LoFTR/releases/download/v1.0/outdoor_ds.ckpt

# Convert to JAX format
python convert_loftr_checkpoint.py \
    --pytorch_ckpt outdoor_ds.ckpt \
    --output loftr_transformer.npz
```

### Step 2: Train Module 1 (Dense Registration)

**Purpose:** Learn dense correspondences with LoFTR initialization

**Configuration:**
```python
# config.py - DENSE_CONFIG
{
    "use_loftr": True,  # âœ“ Use LoFTR architecture
    "loftr_pretrained_ckpt": "./loftr_transformer.npz",  # âœ“ Load LoFTR weights
    "lambda_D": 1.0,  # Only L_D loss
}
```

**Training:**
```bash
python train_dense.py \
    --dataset_root /path/to/fingerprint/data \
    --checkpoint_dir ./checkpoints/dense_reg
```

**Output:** `./checkpoints/dense_reg/dense_reg_ckpt.pkl` (contains TRAINED transformer)

### Step 3: Train Module 2 (Matcher)

**Purpose:** Learn embeddings for verification using Module 1's trained transformer

**Configuration:**
```python
# config.py - MATCH_CONFIG
{
    "use_loftr": True,  # âœ“ Use same architecture as Module 1
    "dense_reg_ckpt": "./checkpoints/dense_reg/dense_reg_ckpt.pkl",  # âœ“ Load from trained Module 1
    "lambda_D": 0.5,
    "lambda_E": 0.1,
    "lambda_A": 1.0,
}
```

**Training:**
```bash
python train_match.py \
    --dataset_root /path/to/fingerprint/data \
    --num_classes 100 \
    --pretrained_ckpt ./checkpoints/dense_reg/dense_reg_ckpt.pkl \
    --checkpoint_dir ./checkpoints/matcher
```

**What happens internally:**
1. Initialize MatcherModel with random weights
2. Load **Module 1's trained transformer** (NOT LoFTR) into both global and local branches
3. Share transformer weights between global/local (faithful to paper)
4. Train with L_D + L_E + L_A losses

**Output:** `./checkpoints/matcher/matcher_ckpt.pkl` (final model for inference)

## âœ… Correct vs âŒ Incorrect

### âœ… Correct (Following IFViT Paper)

```
Module 1: LoFTR init â†’ train with L_D â†’ save trained transformer
Module 2: Load Module 1's trained transformer â†’ train with L_D+L_E+L_A
```

### âŒ Incorrect (What to avoid)

```
Module 1: LoFTR init â†’ train with L_D â†’ save
Module 2: LoFTR init directly â†’ train with L_D+L_E+L_A  â† WRONG!
```

**Why incorrect?** Module 2 discards Module 1's learning, defeats the purpose of two-stage training.

## ðŸ” Verification

Check that Module 2 loaded Module 1 correctly:

```bash
# Verify Module 1 checkpoint
python load_module1_weights.py --module1_ckpt ./checkpoints/dense_reg/dense_reg_ckpt.pkl

# Look for this in training logs:
# "Loading Module 1 Transformer Weights"
# "âœ“ Found transformer: loftr_transformer"
# "âœ“ Copied to loftr_transformer_global"
# "âœ“ Copied to loftr_transformer_local (shared weights)"
```

## ðŸŽ¯ Key Implementation Details

### Transformer Weight Sharing

Module 2 uses `share_global_local=True` by default:

```python
# In train_match.py
params = load_module1_transformer_weights(
    module1_ckpt_path=pretrained_ckpt,
    module2_params=params,
    share_global_local=True  # âœ“ Share Module 1's transformer
)
```

**Why share?**
- IFViT paper uses "the ViTs" (plural = 2 Siamese branches)
- Both branches learn from same Module 1 initialization
- More parameter efficient
- Avoids training separate transformers from scratch

### Module 1 Checkpoint Structure

```
dense_reg_ckpt.pkl/
â”œâ”€â”€ params/
â”‚   â”œâ”€â”€ ResNet18/          # Backbone weights
â”‚   â”œâ”€â”€ loftr_transformer/ # âœ“ This is what Module 2 loads
â”‚   â””â”€â”€ DenseMatchingHead/ # Matching head (not used in Module 2)
â””â”€â”€ metadata/
    â””â”€â”€ config, epoch, etc.
```

### Module 2 Parameter Loading

```
MatcherModel params (before loading):
â”œâ”€â”€ ResNet18/ (random)          â† Not loaded, will train from scratch
â”œâ”€â”€ loftr_transformer_global/   â† Loaded from Module 1
â”œâ”€â”€ loftr_transformer_local/    â† Loaded from Module 1 (same weights)
â”œâ”€â”€ EmbeddingHead/ (random)     â† New, will train from scratch
â””â”€â”€ DenseMatchingHead/ (random) â† For auxiliary L_D loss

After load_module1_transformer_weights():
â”œâ”€â”€ ResNet18/ (still random)
â”œâ”€â”€ loftr_transformer_global/   â† âœ“ Module 1's trained weights
â”œâ”€â”€ loftr_transformer_local/    â† âœ“ Module 1's trained weights (shared)
â”œâ”€â”€ EmbeddingHead/ (still random)
â””â”€â”€ DenseMatchingHead/ (still random)
```

## ðŸ“ Training Logs to Expect

### Module 1 Training
```
Loading LoFTR weights from: ./loftr_transformer.npz
âœ“ Loaded 48 parameter arrays
âœ“ Merged 48/48 LoFTR parameters

DenseRegModel Summary
Total parameters: 12,345,678

Starting training...
Epoch 1/50 | L_D: 2.345
...
```

### Module 2 Training
```
============================================================
Loading Module 1 Transformer Weights
============================================================
Module 1 checkpoint: ./checkpoints/dense_reg/dense_reg_ckpt.pkl
This loads the TRAINED transformer from Module 1,
NOT fresh LoFTR weights (as per IFViT paper)
============================================================

âœ“ Found transformer: loftr_transformer
âœ“ Sharing transformer weights between global and local branches
  âœ“ Copied to loftr_transformer_global
  âœ“ Copied to loftr_transformer_local (shared weights)

âœ“ Successfully loaded 6,543,210 parameters from Module 1
============================================================

MatcherModel Summary
Total parameters: 15,678,901

Starting training...
Epoch 1/40 | L_D: 1.234 | L_E: 0.456 | L_A: 2.345
...
```

## ðŸš¨ Common Mistakes to Avoid

1. **Setting `loftr_pretrained_ckpt` in MATCH_CONFIG**
   - âŒ Module 2 should NOT load LoFTR directly
   - âœ“ Only set this in DENSE_CONFIG for Module 1

2. **Not providing `dense_reg_ckpt`**
   - âŒ Module 2 with random transformer defeats two-stage training
   - âœ“ Always provide trained Module 1 checkpoint

3. **Using different `use_loftr` settings**
   - âŒ Module 1 with LoFTR, Module 2 with generic â†’ incompatible
   - âœ“ Both should use `use_loftr=True` for consistency

4. **Separate global/local transformers**
   - âš ï¸ Not wrong, but less parameter efficient
   - âœ“ Sharing weights is more faithful to paper

## ðŸ“Š Expected Benefits

By following this workflow:

- âœ… **Faster convergence** in Module 2 (builds on Module 1's learning)
- âœ… **Better features** (transformer pretrained on matching task)
- âœ… **Faithful to paper** (exact implementation as described)
- âœ… **Parameter efficient** (shared weights between branches)

## ðŸŽ“ Summary

**The Golden Rule:**

> LoFTR â†’ Module 1 â†’ Module 2
> 
> Each arrow is a checkpoint transfer, never skip Module 1!

Following this workflow ensures your implementation matches the IFViT paper exactly. ðŸŽ¯
