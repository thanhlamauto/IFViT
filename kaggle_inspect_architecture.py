"""
Inspect IFViT architecture and data flow through each component.

This script helps verify the implementation against the paper by:
1. Loading sample data
2. Tracing data through each model component
3. Printing shapes and statistics at each step
4. Comparing with paper specifications

Usage on Kaggle (in a notebook cell):

    !pip install -r /kaggle/working/IFViT/requirements.txt
    %cd /kaggle/working/IFViT
    !python kaggle_inspect_architecture.py

Assumes:
    - This repository is available under /kaggle/working/IFViT
    - Fingerprint datasets are mounted under /kaggle/input
"""

import os
import sys
import jax
import jax.numpy as jnp
import numpy as np
import cv2
from typing import Dict, Tuple

# Ensure local package is importable
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
if ROOT_DIR not in sys.path:
    sys.path.insert(0, ROOT_DIR)

# Import data loading
from data import (
    PaperDatasetRoots,
    build_paper_train_entries,
    load_image,
    normalize_image,
)

# Import model components
sys.path.insert(0, os.path.join(ROOT_DIR, 'ifvit-jax'))
from models import (
    ResNet18,
    SiameseTransformer,
    DenseMatchingHead,
    DenseRegModel,
    MatcherModel,
    EmbeddingHead,
)
from config import DENSE_CONFIG, MATCH_CONFIG


# ============================================================================
# Utility Functions
# ============================================================================

def print_section(title: str, char: str = "=", width: int = 80):
    """Print a formatted section header."""
    print(f"\n{char * width}")
    print(f"{title:^{width}}")
    print(f"{char * width}\n")


def print_tensor_info(name: str, tensor: jnp.ndarray, indent: int = 0):
    """Print tensor shape, dtype, min, max, mean, std."""
    prefix = "  " * indent
    shape_str = str(tensor.shape)
    dtype_str = str(tensor.dtype)
    
    # Convert to numpy for statistics (if tensor is small enough)
    if tensor.size < 1e6:  # Only compute stats for reasonable sizes
        arr = np.array(tensor)
        min_val = float(np.min(arr))
        max_val = float(np.max(arr))
        mean_val = float(np.mean(arr))
        std_val = float(np.std(arr))
        stats_str = f"min={min_val:.4f}, max={max_val:.4f}, mean={mean_val:.4f}, std={std_val:.4f}"
    else:
        stats_str = "(too large for stats)"
    
    print(f"{prefix}{name:30s} | shape={shape_str:25s} | dtype={dtype_str:10s} | {stats_str}")


def compare_with_paper(component: str, actual: Dict, expected: Dict):
    """Compare actual values with paper specifications."""
    print(f"\n  ðŸ“„ Paper Comparison for {component}:")
    for key, expected_val in expected.items():
        actual_val = actual.get(key)
        if actual_val is not None:
            # Convert to string for comparison and formatting
            actual_str = str(actual_val)
            expected_str = str(expected_val)
            match = "âœ“" if actual_str == expected_str else "âœ—"
            print(f"    {match} {key:20s}: actual={actual_str:15s}, expected={expected_str:15s}")
        else:
            expected_str = str(expected_val)
            print(f"    ? {key:20s}: not found, expected={expected_str:15s}")


# ============================================================================
# Module 1: Dense Registration Inspection
# ============================================================================

def inspect_resnet_backbone(img1: jnp.ndarray, img2: jnp.ndarray):
    """Inspect ResNet-18 backbone output."""
    print_section("1. ResNet-18 Backbone (Feature Extraction)")
    
    print("  Input images:")
    print_tensor_info("img1", img1, indent=1)
    print_tensor_info("img2", img2, indent=1)
    
    # Initialize backbone
    backbone = ResNet18()
    rng = jax.random.PRNGKey(0)
    
    # Initialize parameters
    dummy_input = jnp.ones((1, *img1.shape[1:]))
    params = backbone.init(rng, dummy_input, train=False)
    
    # Forward pass
    feat1 = backbone.apply(params, img1, train=False)
    feat2 = backbone.apply(params, img2, train=False)
    
    print("\n  Output feature maps:")
    print_tensor_info("feat1", feat1, indent=1)
    print_tensor_info("feat2", feat2, indent=1)
    
    # Paper comparison
    H_in, W_in = img1.shape[1], img1.shape[2]
    H_out, W_out = feat1.shape[1], feat1.shape[2]
    scale_factor = H_in / H_out
    
    print(f"\n  ðŸ“Š Spatial Resolution:")
    print(f"    Input:  {H_in}x{W_in}")
    print(f"    Output: {H_out}x{W_out}")
    print(f"    Scale factor: {scale_factor:.1f}x (expected: 16x for ResNet-18)")
    
    compare_with_paper(
        "ResNet-18 Backbone",
        {
            "output_channels": feat1.shape[-1],
            "scale_factor": scale_factor,
        },
        {
            "output_channels": "256",
            "scale_factor": "16",
        }
    )
    
    return feat1, feat2, params


def inspect_transformer(feat1: jnp.ndarray, feat2: jnp.ndarray, config: Dict):
    """Inspect Siamese Transformer output."""
    print_section("2. Siamese Transformer (Self & Cross-Attention)")
    
    print("  Input features:")
    print_tensor_info("feat1", feat1, indent=1)
    print_tensor_info("feat2", feat2, indent=1)
    
    # Initialize transformer
    transformer = SiameseTransformer(
        num_layers=config['transformer_layers'],
        num_heads=config['num_heads'],
        hidden_dim=config['hidden_dim'],
        mlp_dim=config['mlp_dim'],
        dropout_rate=config['dropout_rate'],
    )
    
    rng = jax.random.PRNGKey(0)
    dummy_feat1 = jnp.ones((1, *feat1.shape[1:]))
    dummy_feat2 = jnp.ones((1, *feat2.shape[1:]))
    params = transformer.init(rng, dummy_feat1, dummy_feat2, train=False)
    
    # Forward pass
    refined_feat1, refined_feat2 = transformer.apply(
        params, feat1, feat2, train=False
    )
    
    print("\n  Output refined features:")
    print_tensor_info("refined_feat1", refined_feat1, indent=1)
    print_tensor_info("refined_feat2", refined_feat2, indent=1)
    
    # Paper comparison
    compare_with_paper(
        "Siamese Transformer",
        {
            "num_layers": config['transformer_layers'],
            "num_heads": config['num_heads'],
            "hidden_dim": config['hidden_dim'],
            "mlp_dim": config['mlp_dim'],
        },
        {
            "num_layers": "4",
            "num_heads": "8",
            "hidden_dim": "256",
            "mlp_dim": "1024",
        }
    )
    
    return refined_feat1, refined_feat2, params


def inspect_dense_matching_head(feat1: jnp.ndarray, feat2: jnp.ndarray):
    """Inspect Dense Matching Head output."""
    print_section("3. Dense Matching Head (Dual-Softmax)")
    
    print("  Input features:")
    print_tensor_info("feat1", feat1, indent=1)
    print_tensor_info("feat2", feat2, indent=1)
    
    # Initialize matching head
    matching_head = DenseMatchingHead(temperature=0.1)
    
    # Forward pass (no params needed for DenseMatchingHead)
    P, matches = matching_head.apply({}, feat1, feat2)
    
    print("\n  Output:")
    print_tensor_info("P (matching probability matrix)", P, indent=1)
    print_tensor_info("matches (top-K matches)", matches, indent=1)
    
    # Analyze matching matrix
    B, N, M = P.shape
    print(f"\n  ðŸ“Š Matching Matrix Analysis:")
    print(f"    Matrix size: {N}x{M} (N={N} points in img1, M={M} points in img2)")
    print(f"    Top-K matches: {matches.shape[1]}")
    
    # Check dual-softmax properties
    # Each row should sum to ~1 (softmax over columns)
    row_sums = jnp.sum(P, axis=2)
    col_sums = jnp.sum(P, axis=1)
    
    print(f"\n  Dual-Softmax Verification:")
    print_tensor_info("Row sums (should be ~1.0)", row_sums, indent=1)
    print_tensor_info("Column sums (should be ~1.0)", col_sums, indent=1)
    
    return P, matches


def inspect_module1_complete(img1: jnp.ndarray, img2: jnp.ndarray, config: Dict):
    """Inspect complete Module 1 pipeline."""
    print_section("MODULE 1: Dense Registration - Complete Pipeline")
    
    # Initialize model
    model = DenseRegModel(
        image_size=config['image_size'],
        num_transformer_layers=config['transformer_layers'],
        num_heads=config['num_heads'],
        hidden_dim=config['hidden_dim'],
        mlp_dim=config['mlp_dim'],
        dropout_rate=config['dropout_rate'],
        use_loftr=config.get('use_loftr', False),
        attention_type=config.get('attention_type', 'linear'),
    )
    
    rng = jax.random.PRNGKey(0)
    params = model.init(rng, img1, img2, train=False)
    
    # Forward pass
    P, matches, feat1, feat2 = model.apply(params, img1, img2, train=False)
    
    print("  Final outputs:")
    print_tensor_info("P (matching matrix)", P, indent=1)
    print_tensor_info("matches", matches, indent=1)
    print_tensor_info("feat1 (refined)", feat1, indent=1)
    print_tensor_info("feat2 (refined)", feat2, indent=1)
    
    # Count parameters
    param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))
    print(f"\n  ðŸ“Š Model Statistics:")
    print(f"    Total parameters: {param_count:,}")
    
    return P, matches, feat1, feat2, params


# ============================================================================
# Module 2: Matcher Inspection
# ============================================================================

def inspect_embedding_head(feat: jnp.ndarray, embedding_dim: int = 256):
    """Inspect Embedding Head output."""
    print_section("4. Embedding Head (Global Average Pooling + Projection)")
    
    print("  Input feature map:")
    print_tensor_info("feat", feat, indent=1)
    
    # Initialize embedding head
    emb_head = EmbeddingHead(embedding_dim=embedding_dim)
    
    rng = jax.random.PRNGKey(0)
    params = emb_head.init(rng, feat)
    
    # Forward pass
    emb = emb_head.apply(params, feat)
    
    print("\n  Output embedding:")
    print_tensor_info("emb", emb, indent=1)
    
    # Check L2 normalization
    norms = jnp.linalg.norm(emb, axis=-1)
    print(f"\n  L2 Normalization Check:")
    print_tensor_info("Embedding norms (should be ~1.0)", norms, indent=1)
    
    compare_with_paper(
        "Embedding Head",
        {
            "embedding_dim": embedding_dim,
        },
        {
            "embedding_dim": "256",
        }
    )
    
    return emb, params


def inspect_module2_complete(
    img1: jnp.ndarray,
    img2: jnp.ndarray,
    roi1: jnp.ndarray,
    roi2: jnp.ndarray,
    config: Dict
):
    """Inspect complete Module 2 pipeline."""
    print_section("MODULE 2: Fingerprint Matcher - Complete Pipeline")
    
    # Initialize model
    model = MatcherModel(
        image_size=config['image_size'],
        roi_size=config['roi_size'],
        num_transformer_layers=config['transformer_layers'],
        num_heads=config['num_heads'],
        hidden_dim=config['hidden_dim'],
        mlp_dim=config['mlp_dim'],
        dropout_rate=config['dropout_rate'],
        embedding_dim=config['embedding_dim'],
        use_loftr=config.get('use_loftr', False),
        attention_type=config.get('attention_type', 'linear'),
    )
    
    rng = jax.random.PRNGKey(0)
    params = model.init(rng, img1, img2, roi1, roi2, train=False)
    
    # Forward pass
    emb_g1, emb_g2, emb_l1, emb_l2, P, matches = model.apply(
        params, img1, img2, roi1, roi2, train=False
    )
    
    print("  Final outputs:")
    print_tensor_info("emb_g1 (global embedding 1)", emb_g1, indent=1)
    print_tensor_info("emb_g2 (global embedding 2)", emb_g2, indent=1)
    print_tensor_info("emb_l1 (local embedding 1)", emb_l1, indent=1)
    print_tensor_info("emb_l2 (local embedding 2)", emb_l2, indent=1)
    print_tensor_info("P (optional matching matrix)", P, indent=1)
    print_tensor_info("matches", matches, indent=1)
    
    # Compute similarity scores
    from losses import cosine_score, fuse_scores
    
    score_global = cosine_score(emb_g1, emb_g2)
    score_local = cosine_score(emb_l1, emb_l2)
    score_fused = fuse_scores(
        score_global,
        score_local,
        alpha_global=config.get('alpha_global', 0.6),
        alpha_local=config.get('alpha_local', 0.4),
    )
    
    print(f"\n  ðŸ“Š Similarity Scores:")
    # Convert JAX arrays to Python scalars
    score_global_val = float(np.array(score_global)[0]) if score_global.ndim > 0 else float(score_global)
    score_local_val = float(np.array(score_local)[0]) if score_local.ndim > 0 else float(score_local)
    score_fused_val = float(np.array(score_fused)[0]) if score_fused.ndim > 0 else float(score_fused)
    print(f"    Global score: {score_global_val:.4f}")
    print(f"    Local score:  {score_local_val:.4f}")
    print(f"    Fused score:  {score_fused_val:.4f}")
    
    # Count parameters
    param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))
    print(f"\n  ðŸ“Š Model Statistics:")
    print(f"    Total parameters: {param_count:,}")
    
    compare_with_paper(
        "Module 2 Configuration",
        {
            "image_size": config['image_size'],
            "roi_size": config['roi_size'],
            "embedding_dim": config['embedding_dim'],
            "alpha_global": config.get('alpha_global', 0.6),
            "alpha_local": config.get('alpha_local', 0.4),
        },
        {
            "image_size": "224",
            "roi_size": "90",
            "embedding_dim": "256",
            "alpha_global": "0.6",
            "alpha_local": "0.4",
        }
    )
    
    return emb_g1, emb_g2, emb_l1, emb_l2, P, matches, params


# ============================================================================
# Loss Functions Inspection
# ============================================================================

def inspect_losses(
    P: jnp.ndarray,
    emb_g1: jnp.ndarray,
    emb_g2: jnp.ndarray,
    emb_l1: jnp.ndarray,
    emb_l2: jnp.ndarray,
    config: Dict
):
    """Inspect loss function computations."""
    print_section("5. Loss Functions (L_D, L_E, L_A)")
    
    from losses import (
        dense_reg_loss,
        cosine_embedding_loss,
        total_loss_dense,
        total_loss_matcher,
    )
    
    # L_D: Dense correspondence loss
    print("  L_D: Dense Correspondence Loss")
    # Create dummy GT matches (for demonstration)
    B, N, M = P.shape
    H_feat = int(np.sqrt(N))
    W_feat = int(np.sqrt(N))
    
    # Create some dummy correspondences
    dummy_gt_matches = jnp.array([[[10, 10, 12, 12]]])  # (B=1, K=1, 4)
    
    L_D = dense_reg_loss(
        P,
        dummy_gt_matches,
        feature_shape=(H_feat, W_feat)
    )
    print(f"    L_D = {float(L_D):.4f}")
    
    # L_E: Cosine embedding loss
    print("\n  L_E: Cosine Embedding Loss")
    # Genuine pair (label=1)
    labels_genuine = jnp.ones((B,))
    L_E_genuine = cosine_embedding_loss(
        emb_g1, emb_g2, labels_genuine, margin=config.get('embedding_margin', 0.2)
    )
    print(f"    L_E (genuine pair) = {float(L_E_genuine):.4f}")
    
    # Imposter pair (label=-1)
    labels_imposter = -jnp.ones((B,))
    L_E_imposter = cosine_embedding_loss(
        emb_g1, emb_g2, labels_imposter, margin=config.get('embedding_margin', 0.2)
    )
    print(f"    L_E (imposter pair) = {float(L_E_imposter):.4f}")
    
    # Loss weights from paper
    print(f"\n  ðŸ“Š Loss Weights (from paper):")
    print(f"    Î»_D (dense) = {config.get('lambda_D', 0.5)}")
    print(f"    Î»_E (embedding) = {config.get('lambda_E', 0.1)}")
    print(f"    Î»_A (ArcFace) = {config.get('lambda_A', 1.0)}")
    
    compare_with_paper(
        "Loss Weights",
        {
            "lambda_D": config.get('lambda_D', 0.5),
            "lambda_E": config.get('lambda_E', 0.1),
            "lambda_A": config.get('lambda_A', 1.0),
        },
        {
            "lambda_D": "0.5",
            "lambda_E": "0.1",
            "lambda_A": "1.0",
        }
    )


# ============================================================================
# Main Inspection Function
# ============================================================================

def main():
    """Main inspection function."""
    print_section("IFViT Architecture & Data Flow Inspection", char="=", width=80)
    
    # Load sample data
    print("Loading sample data...")
    roots = PaperDatasetRoots()
    train_entries = build_paper_train_entries(roots)
    
    if not train_entries:
        print("âš ï¸  No training entries found. Using dummy data.")
        # Create dummy data
        img1 = jnp.ones((1, 128, 128, 1), dtype=jnp.float32) * 0.5
        img2 = jnp.ones((1, 128, 128, 1), dtype=jnp.float32) * 0.5
        roi1 = jnp.ones((1, 90, 90, 1), dtype=jnp.float32) * 0.5
        roi2 = jnp.ones((1, 90, 90, 1), dtype=jnp.float32) * 0.5
    else:
        # Load first two images
        entry1 = train_entries[0]
        entry2 = train_entries[1] if len(train_entries) > 1 else train_entries[0]
        
        img1_raw = load_image(entry1.path)
        img2_raw = load_image(entry2.path)
        
        img1_norm = normalize_image(img1_raw)
        img2_norm = normalize_image(img2_raw)
        
        # normalize_image returns (H, W) float32 in [0, 1]
        # Resize to expected sizes
        img1_resized = cv2.resize(img1_norm, (128, 128), interpolation=cv2.INTER_LINEAR)
        img2_resized = cv2.resize(img2_norm, (128, 128), interpolation=cv2.INTER_LINEAR)
        
        # Add batch and channel dimensions: (H, W) -> (1, H, W, 1)
        img1 = jnp.array(img1_resized[None, ..., None], dtype=jnp.float32)
        img2 = jnp.array(img2_resized[None, ..., None], dtype=jnp.float32)
        
        # Create dummy ROIs (in real scenario, these come from FingerNet)
        roi1_resized = cv2.resize(img1_norm, (90, 90), interpolation=cv2.INTER_LINEAR)
        roi2_resized = cv2.resize(img2_norm, (90, 90), interpolation=cv2.INTER_LINEAR)
        roi1 = jnp.array(roi1_resized[None, ..., None], dtype=jnp.float32)
        roi2 = jnp.array(roi2_resized[None, ..., None], dtype=jnp.float32)
        
        print(f"  Loaded: {os.path.basename(entry1.path)}")
        print(f"  Loaded: {os.path.basename(entry2.path)}")
    
    # ========================================================================
    # Module 1 Inspection
    # ========================================================================
    
    print_section("MODULE 1: Dense Registration", char="=", width=80)
    
    # Step-by-step inspection
    feat1, feat2, backbone_params = inspect_resnet_backbone(img1, img2)
    refined_feat1, refined_feat2, transformer_params = inspect_transformer(
        feat1, feat2, DENSE_CONFIG
    )
    P, matches = inspect_dense_matching_head(refined_feat1, refined_feat2)
    
    # Complete pipeline
    P_full, matches_full, feat1_full, feat2_full, params_full = inspect_module1_complete(
        img1, img2, DENSE_CONFIG
    )
    
    # ========================================================================
    # Module 2 Inspection
    # ========================================================================
    
    print_section("MODULE 2: Fingerprint Matcher", char="=", width=80)
    
    # Resize images for Module 2
    img1_np = np.array(img1[0, ..., 0])  # (128, 128)
    img2_np = np.array(img2[0, ..., 0])  # (128, 128)
    img1_module2_resized = cv2.resize(img1_np, (224, 224), interpolation=cv2.INTER_LINEAR)
    img2_module2_resized = cv2.resize(img2_np, (224, 224), interpolation=cv2.INTER_LINEAR)
    img1_module2 = jnp.array(img1_module2_resized[None, ..., None], dtype=jnp.float32)
    img2_module2 = jnp.array(img2_module2_resized[None, ..., None], dtype=jnp.float32)
    
    # Embedding head inspection
    emb1, emb_params = inspect_embedding_head(feat1, embedding_dim=256)
    
    # Complete Module 2 pipeline
    emb_g1, emb_g2, emb_l1, emb_l2, P_m2, matches_m2, params_m2 = inspect_module2_complete(
        img1_module2, img2_module2, roi1, roi2, MATCH_CONFIG
    )
    
    # ========================================================================
    # Loss Functions Inspection
    # ========================================================================
    
    inspect_losses(P_full, emb_g1, emb_g2, emb_l1, emb_l2, MATCH_CONFIG)
    
    # ========================================================================
    # Summary
    # ========================================================================
    
    print_section("Summary & Paper Verification", char="=", width=80)
    
    print("""
  âœ… Architecture Components Verified:
    1. ResNet-18 backbone: extracts features at 1/16 resolution
    2. Siamese Transformer: self-attention + cross-attention
    3. Dense Matching Head: dual-softmax correlation
    4. Embedding Head: global pooling + L2 normalization
    
  âœ… Module 1 (Dense Registration):
    - Input: 128x128 images
    - Output: Matching probability matrix P
    - Loss: L_D only
    
  âœ… Module 2 (Fingerprint Matcher):
    - Input: 224x224 images + 90x90 ROIs
    - Output: Global + Local embeddings
    - Losses: L_D (0.5) + L_E (0.1) + L_A (1.0)
    
  ðŸ“„ Paper Reference: IFViT (2404.08237v1)
    """)
    
    print("=" * 80)


if __name__ == "__main__":
    main()

